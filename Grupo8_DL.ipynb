{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "rfrxIdGw26Ym",
        "HllAJ-x-S7SW",
        "ldoxm0hTTGSb",
        "wi9T7ZWwTJco",
        "964_M2FQTK0T",
        "GgokzsK6KQAc"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/drawnator/Password-optimizer/blob/main/Grupo8_DL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ferramenta Otimizadora De Aleatoriedade em senhas\n",
        "---\n",
        "Modelo de linguagem que auxilia a tornar senhas mais fortes, com base em critérios de fortificação de senhas como o [zxcvbn](https://github.com/dropbox/zxcvbn) e em [senhas vazadas da plataforma rockyou](https://github.com/brannondorsey/naive-hashcat/releases/download/data/rockyou.txt).\n",
        "\n",
        "A aplicação prática do modelo é analisar a senha de entrada, identificar se ela é fraca ou forte segundo critérios predefinidos, se é uma senha comum (presentes na base de dados de senhas vazadas) ou não, e, por fim, dar sugestões pontuais de como tornar a senha mais improvável de adivinhar.\n",
        "\n",
        "Assuntos:\n",
        "- Análise de frequência\n",
        "- Masked language model\n",
        "\n",
        "Tecnologias utilizadas:\n",
        "- RNN\n",
        "- Árvores de decisão\n",
        "- ZXCVBN\n",
        "- Bert?\n",
        "- tsne?\n",
        "- Princípios de senha aleatória"
      ],
      "metadata": {
        "id": "hX_YXyR6EBrn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Preparando o ambiente de execução (imports necessários)"
      ],
      "metadata": {
        "id": "Vzuy8RtdHI4W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install zxcvbn"
      ],
      "metadata": {
        "id": "D9_8SvpSHnb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Dense, Dropout, SimpleRNN\n",
        "from keras.layers import Bidirectional\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import Sequence\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "import pickle"
      ],
      "metadata": {
        "id": "_RQAO_HBKYuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from zxcvbn import zxcvbn"
      ],
      "metadata": {
        "id": "pXbXdDg2PX42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm"
      ],
      "metadata": {
        "id": "y8J5X0ZQxwMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparando os dados"
      ],
      "metadata": {
        "id": "rPVu61au2S4B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "path = \"rockyou.txt\"\n",
        "path = '/content/drive/MyDrive/my_models/rockyout.txt'\n"
      ],
      "metadata": {
        "id": "OG73iqLUvmIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RECOMENDO FORTEMENTE BAIXAR E ARRASTAR MANUALMENTE ATÉ ARTIGOS, ISSO AQUI DEMORA MT \\/\n",
        "url = \"https://github.com/brannondorsey/naive-hashcat/releases/download/data/rockyou.txt\"\n",
        "#download from github using wget\n",
        "!wget {url} -O rockyou.txt"
      ],
      "metadata": {
        "id": "eT4e-TD-2Was"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"rockyou.txt\", \"r\", encoding='latin-1') as f:\n",
        "  df = pd.DataFrame(f.readlines(), columns=['password'])"
      ],
      "metadata": {
        "id": "No5klVbO2nvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['password'] = df['password'].str.replace('\\n', '')"
      ],
      "metadata": {
        "id": "zUtJslgF2pY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if not os.path.exists(\"rockyou.csv\"):\n",
        "#   df.to_csv(\"rockyou.csv\")"
      ],
      "metadata": {
        "id": "S4nnAppL2t0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[df['password'].str.len() > 1]\n",
        "display(df.shape)"
      ],
      "metadata": {
        "id": "-Wi_Kb2eLC5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "20iT9wPq2sZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['password_length'] = df['password'].str.len()\n",
        "\n",
        "length_counts = df['password_length'].value_counts().sort_index()\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.bar(length_counts.index, length_counts.values)\n",
        "plt.xscale('log')\n",
        "plt.yscale('log')\n",
        "plt.xlabel('Password Length (log scale)')\n",
        "plt.ylabel('Number of Passwords (log scale)')\n",
        "plt.title('Distribution of Password Lengths (Logarithmic Scale)')\n",
        "plt.grid(True, which=\"both\", ls=\"--\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_Bdpxxb5N_jo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Métodos de avaliar a qualidade de uma senha"
      ],
      "metadata": {
        "id": "rGigW6AgJasc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RNN\n",
        "\n",
        "Previsibilidade de caracteres, com base em uma parte da senha o quão fácil é prever o resto dela?\n"
      ],
      "metadata": {
        "id": "8WOA63DN2CmI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenização\n",
        "\n",
        "Transformando os caracteres de uma senha em tokens para que eles estejam no formato adequado para o processamento."
      ],
      "metadata": {
        "id": "0f0dF3CQNf5f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_chars = sorted(list(set(''.join(df['password']))))\n",
        "char_to_int = {char: i for i, char in enumerate(all_chars)}\n",
        "int_to_char = {i: char for char, i in char_to_int.items()}"
      ],
      "metadata": {
        "id": "fVY6zfNDQyRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "char_to_int['[MASK]'] = len(char_to_int)\n",
        "char_to_int['[CLS]'] = len(char_to_int)\n",
        "char_to_int['[SEP]'] = len(char_to_int)\n",
        "char_to_int['[PAD]'] = len(char_to_int)\n",
        "int_to_char = {i: char for char, i in char_to_int.items()}"
      ],
      "metadata": {
        "id": "i0oOgtFOT93d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNTokenizer():\n",
        "  def __init__(self, char_to_int,int_to_char,max_length = 32):\n",
        "    self.char_to_int = char_to_int\n",
        "    self.int_to_char = int_to_char\n",
        "    self.mask_token_id = char_to_int['[MASK]']\n",
        "    self.cls_token_id = char_to_int['[CLS]']\n",
        "    self.sep_token_id = char_to_int['[SEP]']\n",
        "    self.pad_token_id = char_to_int['[PAD]']\n",
        "    self.max_length = max_length\n",
        "    self.vocab_size = len(char_to_int)\n",
        "\n",
        "  def __call__(self,text):\n",
        "    token = [self.pad_token_id] * self.max_length\n",
        "    text = f\"{text:>{self.max_length}}\"\n",
        "    # print(text)\n",
        "    for i in range(self.max_length):\n",
        "      token[-i] = char_to_int[text[-i]]\n",
        "\n",
        "    # for i in range(self.max_length):\n",
        "    #   if (i < len(text)):\n",
        "    #     token.append(char_to_int[text[i]])\n",
        "    #   else:\n",
        "    #     token.append(char_to_int['[PAD]'])\n",
        "\n",
        "    return token"
      ],
      "metadata": {
        "id": "FVDjp6dwT5f3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rnntokenizer = RNNTokenizer(char_to_int,int_to_char)"
      ],
      "metadata": {
        "id": "sMTlDRQeUAa6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(rnntokenizer.vocab_size)"
      ],
      "metadata": {
        "id": "w5YnxiDRMbBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNDataloader(Sequence):\n",
        "  def __init__(self,dataframe,tokenizer,batch_size=100):\n",
        "    self.dataframe = dataframe\n",
        "    self.tokenizer = tokenizer\n",
        "    self.batch_size = batch_size\n",
        "\n",
        "  def mask_and_tokens(self,password):\n",
        "      i = np.random.randint(1, min(len(password),self.tokenizer.max_length))\n",
        "      label = self.tokenizer.char_to_int[password[i]]\n",
        "      input_seq = self.tokenizer(password[:i])\n",
        "\n",
        "      # input_seq += [self.tokenizer.mask_token_id]\n",
        "      # input_seq += [self.tokenizer.pad_token_id] * (self.tokenizer.max_length - len(input_seq))\n",
        "      return input_seq,label\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.dataframe) // self.batch_size\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    batch = self.dataframe[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "    batch_inputs = []\n",
        "    batch_labels = []\n",
        "    for password in batch:\n",
        "      input_seq, label = self.mask_and_tokens(password)\n",
        "      batch_inputs.append(input_seq)\n",
        "      batch_labels.append(label)\n",
        "    return np.array(batch_inputs), np.array(batch_labels)"
      ],
      "metadata": {
        "id": "-vnhBPtAAx97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = RNNDataloader(df['password'],rnntokenizer,1)"
      ],
      "metadata": {
        "id": "cKoiosZFA06E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"password\"][0]"
      ],
      "metadata": {
        "id": "NNvMtosUBk2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader[0]"
      ],
      "metadata": {
        "id": "58BckWpOSTK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# drive.mount('/content/drive')\n",
        "\n",
        "# char_to_int = '/content/drive/MyDrive/my_models/char_to_int.pkl'\n",
        "# int_to_char = '/content/drive/MyDrive/my_models/int_to_char.pkl'\n",
        "\n",
        "# with open(char_to_int, 'wb') as f:\n",
        "#     pickle.dump(char_to_int, f)\n",
        "\n",
        "# with open(int_to_char, 'wb') as f:\n",
        "#     pickle.dump(int_to_char, f)"
      ],
      "metadata": {
        "id": "hC4ssj5vnzLe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# drive.mount('/content/drive')\n",
        "# with open(char_to_int, 'rb') as f:\n",
        "#     char_to_int = pickle.load(f)\n",
        "\n",
        "# with open(int_to_char, 'rb') as f:\n",
        "#     int_to_char = pickle.load(f)"
      ],
      "metadata": {
        "id": "t0vOPHedocf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dividindo os dados em conjuntos de treino, validação e teste"
      ],
      "metadata": {
        "id": "GkaguFhoY-vs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "def create_train_val_test_arrays(df, dataloader, tokenizer, test_size=0.2, val_size=0.5,):\n",
        "    train_df, temp_df = train_test_split(df, test_size=test_size, random_state=42)\n",
        "    val_df, test_df = train_test_split(temp_df, test_size=val_size, random_state=42)\n",
        "\n",
        "    train_dataloader = dataloader(train_df, tokenizer,batch_size=1000)\n",
        "    val_dataloader = dataloader(val_df, tokenizer,batch_size=1000)\n",
        "    test_dataloader = dataloader(test_df, tokenizer,batch_size=1000)\n",
        "\n",
        "    return train_dataloader, val_dataloader, test_dataloader"
      ],
      "metadata": {
        "id": "0bE3jvF_fxIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader,val_dataloader,test_dataloader = create_train_val_test_arrays(df[\"password\"], RNNDataloader, rnntokenizer)"
      ],
      "metadata": {
        "id": "2IHv8Tv8gndR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definindo o modelo"
      ],
      "metadata": {
        "id": "zwoaQJ2obmui"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### training model"
      ],
      "metadata": {
        "id": "V-gx8rSL-Q6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#https://colab.research.google.com/drive/1mts5E3yAd1irLzS7Ei6UtwbG773C87DB?usp=sharing\n",
        "model = Sequential([\n",
        "    Embedding(\n",
        "        input_dim=rnntokenizer.vocab_size,\n",
        "        output_dim=100,\n",
        "        input_shape=(rnntokenizer.max_length,)),\n",
        "    Bidirectional(SimpleRNN(64, return_sequences=True)),\n",
        "    Dropout(0.3),\n",
        "    Bidirectional(SimpleRNN(64, return_sequences=False)),\n",
        "    Dropout(0.2),\n",
        "    Dense(100, activation='relu'),\n",
        "    Dropout(0.1),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.05),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(rnntokenizer.vocab_size, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "M9EaU8RZc8cW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    train_dataloader,\n",
        "    validation_data=(val_dataloader),\n",
        "    epochs=5,\n",
        "    batch_size=128,#256\n",
        "    verbose=1)"
      ],
      "metadata": {
        "id": "3no88mxsehfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8gIRaA3JMvJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "model_save_path = '/content/drive/MyDrive/my_models/rnn_password_model5.keras'\n",
        "\n",
        "import os\n",
        "os.makedirs(os.path.dirname(model_save_path), exist_ok=True)\n",
        "\n",
        "model.save(model_save_path)"
      ],
      "metadata": {
        "id": "sb39u4HsKfdM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### load model"
      ],
      "metadata": {
        "id": "E0uf8Qeh-OA6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import tensorflow as tf\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "model_load_path = '/content/drive/MyDrive/my_models/rnn_password_model5.keras'\n",
        "\n",
        "model = tf.keras.models.load_model(model_load_path)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "k63sWfigRrVJ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## métricas de modelo"
      ],
      "metadata": {
        "id": "BwqPjfO0eG07"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RnnMetrics():\n",
        "  def __init__(self,model:tf.keras.models.Model,tokenizer:RNNTokenizer):\n",
        "    self.model = model\n",
        "    self.tokenizer = tokenizer\n",
        "\n",
        "  def preencher_senha(self,senha_base):\n",
        "    while len(senha_base) < self.tokenizer.max_length:\n",
        "      senha_base += self.prever_proxima(senha_base)\n",
        "    return senha_base\n",
        "\n",
        "  def prever_proxima(self,palavras):\n",
        "      seq = self.tokenizer(palavras)\n",
        "      seq = np.array([seq])\n",
        "      pred = self.model.predict(seq, verbose=0)\n",
        "      idx = np.argmax(pred)\n",
        "      return self.tokenizer.int_to_char[idx]\n",
        "\n",
        "  def get_probs(self,palavra,temperatura=1.0):\n",
        "    seq = self.tokenizer(palavra)\n",
        "    seq = np.array([seq])\n",
        "    pred = self.model.predict(seq, verbose=0)\n",
        "    logits = np.log(pred + 1e-9) / temperatura  # evita log(0)\n",
        "    exp_logits = np.exp(logits)\n",
        "    probs = exp_logits / np.sum(exp_logits)\n",
        "    return probs\n",
        "\n",
        "  def get_char_rarity(self,probs,char):\n",
        "    char_token = self.tokenizer.char_to_int[char]\n",
        "    smaller_than = np.where(probs < probs[0][char_token])[0]\n",
        "    return smaller_than.shape[0]/self.tokenizer.vocab_size\n",
        "\n",
        "  def get_rarity(self,password):\n",
        "    rarity = []\n",
        "    for part in range(len(password)):\n",
        "      probs = self.get_probs(password[:part])\n",
        "      rarity.append(self.get_char_rarity(probs,password[part]))\n",
        "    return rarity\n",
        "\n",
        "  def get_char_proportional_probability(self,probs,char):\n",
        "    char_token = self.tokenizer.char_to_int[char]\n",
        "    return probs[0][char_token]\n",
        "\n",
        "  def get_proportional_probability(self,password):\n",
        "    rarity = []\n",
        "    for part in range(len(password)):\n",
        "      probs = self.get_probs(password[:part])\n",
        "      rarity.append(self.get_char_proportional_probability(probs,password[part]))\n",
        "    return rarity\n",
        "\n",
        "  def get_weighted_scores(self,password,weight=[1,1]):\n",
        "    rarity = self.get_rarity(password)\n",
        "    proportional_probability = self.get_proportional_probability(password)\n",
        "    return (np.array(rarity) * weight[0] + np.array(proportional_probability) * weight[1])/sum(weight)\n",
        "\n",
        "  def get_rarer_alternatives(self,password):\n",
        "    probs = self.get_probs(password[:len(password)])\n",
        "    rarer_alternatives = np.where(probs[0] < probs[0][self.tokenizer.char_to_int[password[-1]]])[0]\n",
        "    return [self.tokenizer.int_to_char[i] for i in rarer_alternatives]\n",
        "\n"
      ],
      "metadata": {
        "id": "dg_mQylPe4yU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rnnmetrics = RnnMetrics(model,rnntokenizer)"
      ],
      "metadata": {
        "id": "eQfsKREqffhV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rnnmetrics.get_probs(\"i\")"
      ],
      "metadata": {
        "id": "ZOO2whVgDV_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rnnmetrics.preencher_senha(\"passwo\")"
      ],
      "metadata": {
        "id": "mERAKpA8SoMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rnnmetrics.get_proportional_probability(\"password12345\")"
      ],
      "metadata": {
        "id": "uzg1wOQveyd7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### visualizar métricas do modelo"
      ],
      "metadata": {
        "id": "jJaEdCejooeB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import math\n",
        "import warnings\n",
        "\n",
        "def plot_rnn_probability_heatmap(probabilities, tokenizer):\n",
        "    with warnings.catch_warnings():\n",
        "        warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "        vocab_size = len(tokenizer.char_to_int)\n",
        "        grid_size = math.ceil(math.sqrt(vocab_size))\n",
        "        padded_probs = list(probabilities[0]) + [0.0] * (grid_size * grid_size - vocab_size)\n",
        "\n",
        "        heatmap_data = np.array(padded_probs).reshape(grid_size, grid_size)\n",
        "\n",
        "        plt.figure(figsize=(grid_size * 0.5, grid_size * 0.5))\n",
        "        plt.imshow(heatmap_data, cmap='viridis', aspect='auto', vmin=0.0, vmax=1.0)\n",
        "        plt.colorbar(label='Probability')\n",
        "\n",
        "        for i in range(grid_size):\n",
        "            for j in range(grid_size):\n",
        "                index = i * grid_size + j\n",
        "                if index < vocab_size:\n",
        "                    char = tokenizer.int_to_char[index]\n",
        "                    prob = heatmap_data[i, j]\n",
        "                    text_color = 'white' if prob < 0.5 else 'black'\n",
        "                    plt.text(j, i, char, ha='center', va='center', color=text_color, fontsize=16)\n",
        "\n",
        "        plt.title(\"RNN Probability Heatmap\")\n",
        "        plt.yticks([])\n",
        "        plt.xticks([])\n",
        "        plt.tight_layout()\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "xO2DYUvpPXqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_rnn_probability_heatmap(rnnmetrics.get_probs(\"passwor\"), rnntokenizer)"
      ],
      "metadata": {
        "id": "K5mZwQ9EXCUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import math\n",
        "import warnings\n",
        "\n",
        "def plot_rnn_rank_heatmap(probabilities, tokenizer):\n",
        "    with warnings.catch_warnings():\n",
        "        warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "        vocab_size = len(tokenizer.char_to_int)\n",
        "        grid_size = math.ceil(math.sqrt(vocab_size))\n",
        "        padded_probs = list(probabilities[0]) + [0.0] * (grid_size * grid_size - vocab_size)\n",
        "\n",
        "        indices  = np.argsort(padded_probs)\n",
        "        rank = np.zeros_like(indices)\n",
        "        rank[indices] = np.arange(len(indices))\n",
        "        rank = rank/rank.shape[0]\n",
        "\n",
        "        heatmap_data = np.array(rank).reshape(grid_size, grid_size)\n",
        "\n",
        "        plt.figure(figsize=(grid_size * 0.5, grid_size * 0.5))\n",
        "        plt.imshow(heatmap_data, cmap='viridis', aspect='auto')\n",
        "        plt.colorbar(label='Probability')\n",
        "\n",
        "        for i in range(grid_size):\n",
        "            for j in range(grid_size):\n",
        "                index = i * grid_size + j\n",
        "                if index < vocab_size:\n",
        "                    char = tokenizer.int_to_char[index]\n",
        "                    prob = heatmap_data[i, j]\n",
        "                    text_color = 'white' if prob < 0.5 else 'black'\n",
        "                    plt.text(j, i, char, ha='center', va='center', color=text_color, fontsize=16)\n",
        "\n",
        "        plt.title(\"RNN Probability Heatmap\")\n",
        "        plt.yticks([])\n",
        "        plt.xticks([])\n",
        "        plt.tight_layout()\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "h4TXsLmjXs98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_rnn_rank_heatmap(rnnmetrics.get_probs(\"passwor\"), rnntokenizer)"
      ],
      "metadata": {
        "id": "VrIgvlj6Xvjs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import math\n",
        "import warnings\n",
        "\n",
        "def plot_rnn_combined_heatmap(probabilities, tokenizer):\n",
        "    with warnings.catch_warnings():\n",
        "        warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "        vocab_size = len(tokenizer.char_to_int)\n",
        "        grid_size = math.ceil(math.sqrt(vocab_size))\n",
        "        padded_probs = list(probabilities[0]) + [0.0] * (grid_size * grid_size - vocab_size)\n",
        "\n",
        "        indices  = np.argsort(padded_probs)\n",
        "        rank = np.zeros_like(indices)\n",
        "        rank[indices] = np.arange(len(indices))\n",
        "        rank = rank/rank.shape[0]\n",
        "\n",
        "        rank = rank+padded_probs\n",
        "\n",
        "        heatmap_data = np.array(rank).reshape(grid_size, grid_size)\n",
        "\n",
        "        plt.figure(figsize=(grid_size * 0.5, grid_size * 0.5))\n",
        "        plt.imshow(heatmap_data, cmap='viridis', aspect='auto')\n",
        "        plt.colorbar(label='Probability')\n",
        "\n",
        "        for i in range(grid_size):\n",
        "            for j in range(grid_size):\n",
        "                index = i * grid_size + j\n",
        "                if index < vocab_size:\n",
        "                    char = tokenizer.int_to_char[index]\n",
        "                    prob = heatmap_data[i, j]\n",
        "                    text_color = 'white' if prob < 0.5 else 'black'\n",
        "                    plt.text(j, i, char, ha='center', va='center', color=text_color, fontsize=16)\n",
        "\n",
        "        plt.title(\"RNN Probability Heatmap\")\n",
        "        plt.yticks([])\n",
        "        plt.xticks([])\n",
        "        plt.tight_layout()\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "N6NX1LNIUZff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_rnn_combined_heatmap(rnnmetrics.get_probs(\"passwor\"), rnntokenizer)"
      ],
      "metadata": {
        "id": "yUG7Cot9VDCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ipywidgets as widgets\n",
        "from ipywidgets import interact, interactive, fixed, interact_manual\n",
        "from IPython.display import display\n",
        "\n",
        "# Define the function to be called by the interactive widget\n",
        "def interactive_rnn_heatmap(password):\n",
        "    if password: # Only plot if there is text\n",
        "        probs = rnnmetrics.get_probs(password)\n",
        "        plot_rnn_combined_heatmap(probs, rnntokenizer)\n",
        "    else:\n",
        "        print(\"Please enter a password to see the heatmap.\")\n",
        "\n",
        "# Create an interactive widget\n",
        "password_input = widgets.Text(\n",
        "    value='password123',\n",
        "    placeholder='Enter password',\n",
        "    description='Password:',\n",
        "    disabled=False\n",
        ")\n",
        "\n",
        "# Link the widget to the function\n",
        "interact(interactive_rnn_heatmap, password=password_input);"
      ],
      "metadata": {
        "id": "BWxU-7afSPmU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def plot_score_heatmap(password, scores):\n",
        "    heatmap_data = np.array(scores).reshape(1, -1)\n",
        "\n",
        "    plt.figure(figsize=(len(password) * 0.5, 1))\n",
        "    plt.imshow(heatmap_data, cmap='viridis', aspect='auto', vmin=0.0, vmax=1)\n",
        "    plt.colorbar(label='Score (0-1)')\n",
        "\n",
        "    for i in range(len(password)):\n",
        "        plt.text(i, 0, password[i], ha='center', va='center', color='white' if heatmap_data[0, i] < 0.5 else 'black')\n",
        "\n",
        "    plt.title(f\"{password}\")\n",
        "    plt.yticks([])\n",
        "    plt.xticks([])\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "Gu4CjsL7kDKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "password = \"password12345\"\n",
        "rarity = rnnmetrics.get_rarity(password)\n",
        "plot_score_heatmap(password, rarity)\n",
        "print(sum(rarity)/len(rarity))\n",
        "\n",
        "password = \"ThisIsAStrongPassword123!\"\n",
        "rarity = rnnmetrics.get_rarity(password)\n",
        "plot_score_heatmap(password, rarity)\n",
        "print(sum(rarity)/len(rarity))\n",
        "\n",
        "password = \"aNXeuj~ouU\"\n",
        "rarity = rnnmetrics.get_rarity(password)\n",
        "plot_score_heatmap(password, rarity)\n",
        "print(sum(rarity)/len(rarity))"
      ],
      "metadata": {
        "id": "564qM75qom-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "password = \"password12345\"\n",
        "proportional = rnnmetrics.get_proportional_probability(password)\n",
        "plot_score_heatmap(password, proportional)\n",
        "print(sum(proportional)/len(proportional))\n",
        "\n",
        "password = \"ThisIsAStrongPassword123!\"\n",
        "proportional = rnnmetrics.get_proportional_probability(password)\n",
        "plot_score_heatmap(password, proportional)\n",
        "print(sum(proportional)/len(proportional))\n",
        "\n",
        "\n",
        "password = \"aNXeuj~ouU\"\n",
        "proportional = rnnmetrics.get_proportional_probability(password)\n",
        "plot_score_heatmap(password, proportional)\n",
        "print(sum(proportional)/len(proportional))\n"
      ],
      "metadata": {
        "id": "9Xyo-RHjnZCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "password = \"password12345\"\n",
        "weighted = rnnmetrics.get_weighted_scores(password,[1,1])\n",
        "plot_score_heatmap(password,weighted )\n",
        "print(sum(weighted)/len(weighted))\n",
        "\n",
        "password = \"ThisIsAStrongPassword123!\"\n",
        "weighted = rnnmetrics.get_weighted_scores(password,[1,1])\n",
        "plot_score_heatmap(password,weighted )\n",
        "print(sum(weighted)/len(weighted))\n",
        "\n",
        "password = \"aNXeuj~ouU\"\n",
        "weighted = rnnmetrics.get_weighted_scores(password,[1,1])\n",
        "plot_score_heatmap(password,weighted )\n",
        "print(sum(weighted)/len(weighted))"
      ],
      "metadata": {
        "id": "VxLH9vZjl6sL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bert"
      ],
      "metadata": {
        "id": "rfrxIdGw26Ym"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## tokenização\n"
      ],
      "metadata": {
        "id": "HllAJ-x-S7SW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertTokenizer, TFDistilBertForMaskedLM\n",
        "from transformers import BertTokenizer\n",
        "from transformers import TFDistilBertForSequenceClassification"
      ],
      "metadata": {
        "id": "2Bmkoha5o5eO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "berttokenizer = DistilBertTokenizer.from_pretrained(\n",
        "    'distilbert-base-cased',\n",
        "    # do_basic_tokenize=False,\n",
        "    # do_lower_case=False,\n",
        "    # split_special_tokens=True,\n",
        "    )\n",
        "seq_length = 32"
      ],
      "metadata": {
        "id": "UY4XkQeITbrR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token = berttokenizer(\"aNXeuj~ouU\",\n",
        "                      padding='max_length',\n",
        "                      truncation=True,\n",
        "                      max_length=seq_length,\n",
        "                      split_special_tokens=True\n",
        "                      )\n",
        "token"
      ],
      "metadata": {
        "id": "y9h2cxPNZwDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of decoding each token individually\n",
        "decoded_tokens = [berttokenizer.decode([token_id]) for token_id in token['input_ids']]\n",
        "print(decoded_tokens)"
      ],
      "metadata": {
        "id": "lCu77zC3UwAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BertDataloader(Sequence):\n",
        "  def __init__(self,dataframe,tokenizer,seq_length=32,batch_size=100):\n",
        "    self.dataframe = dataframe\n",
        "    self.tokenizer = tokenizer\n",
        "    self.batch_size = batch_size\n",
        "    self.seq_length = seq_length\n",
        "\n",
        "  def mask_and_tokens(self,password):\n",
        "      token = self.tokenizer(password,\n",
        "                              padding='max_length',\n",
        "                              truncation=True,\n",
        "                              max_length=self.seq_length,\n",
        "                              split_special_tokens=True\n",
        "                              )\n",
        "      sep_index = token['input_ids'].index(self.tokenizer.sep_token_id)\n",
        "      i = np.random.randint(1, sep_index)\n",
        "\n",
        "      label = np.full(self.seq_length, 0, dtype=np.int32)\n",
        "      label[i] = token['input_ids'][i]\n",
        "      token['input_ids'][i] = self.tokenizer.mask_token_id\n",
        "\n",
        "      return token,label\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.dataframe) // self.batch_size\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    batch = self.dataframe[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "    batch_inputs = {'input_ids': [], 'attention_mask': []}\n",
        "    batch_labels = []\n",
        "    for password in batch:\n",
        "      input_seq, label = self.mask_and_tokens(password)\n",
        "      batch_inputs['input_ids'].append(input_seq['input_ids'])\n",
        "      batch_inputs['attention_mask'].append(input_seq['attention_mask'])\n",
        "      batch_labels.append(label)\n",
        "\n",
        "    # Convert lists to TensorFlow tensors\n",
        "    batch_inputs['input_ids'] = tf.constant(batch_inputs['input_ids'], dtype=tf.int32)\n",
        "    batch_inputs['attention_mask'] = tf.constant(batch_inputs['attention_mask'], dtype=tf.int32)\n",
        "    batch_labels = tf.constant(batch_labels, dtype=tf.int32)\n",
        "\n",
        "    return batch_inputs, batch_labels"
      ],
      "metadata": {
        "id": "Mb5h_k7fS2Ow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_bert_dataset(dataframe, tokenizer, seq_length, batch_size):\n",
        "    input_ids_list = []\n",
        "    attention_mask_list = []\n",
        "    labels_list = []\n",
        "\n",
        "    for password in tqdm.tqdm(dataframe):\n",
        "      token = tokenizer(password,\n",
        "                        padding='max_length',\n",
        "                        truncation=True,\n",
        "                        max_length=seq_length,\n",
        "                        split_special_tokens=True\n",
        "                        )\n",
        "\n",
        "      sep_index = token['input_ids'].index(tokenizer.sep_token_id)\n",
        "      if sep_index > 1:\n",
        "          i = np.random.randint(1, sep_index)\n",
        "      else:\n",
        "            continue\n",
        "\n",
        "      label = np.full(seq_length, -100, dtype=np.int32) #uhoh...\n",
        "      label[i] = token['input_ids'][i]\n",
        "      token['input_ids'][i] = tokenizer.mask_token_id\n",
        "\n",
        "      input_ids_list.append(token['input_ids'])\n",
        "      attention_mask_list.append(token['attention_mask'])\n",
        "      labels_list.append(label)\n",
        "\n",
        "    input_ids_tensor = tf.constant(input_ids_list, dtype=tf.int32)\n",
        "    attention_mask_tensor = tf.constant(attention_mask_list, dtype=tf.int32)\n",
        "    labels_tensor = tf.constant(labels_list, dtype=tf.int32)\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(({'input_ids': input_ids_tensor,\n",
        "                                                   'attention_mask': attention_mask_tensor},\n",
        "                                                  labels_tensor))\n",
        "\n",
        "    dataset = dataset.shuffle(buffer_size=len(input_ids_list)).batch(batch_size)\n",
        "\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "C2HP9CRcwwrd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dividindo os dados em conjuntos de treino, validação e teste"
      ],
      "metadata": {
        "id": "ldoxm0hTTGSb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "def create_train_val_test_arrays(df, dataloader, tokenizer, test_size=0.2, val_size=0.5,):\n",
        "    train_df, temp_df = train_test_split(df, test_size=test_size, random_state=42)\n",
        "    val_df, test_df = train_test_split(temp_df, test_size=val_size, random_state=42)\n",
        "\n",
        "    train_dataloader = dataloader(train_df, tokenizer,batch_size=1000)\n",
        "    val_dataloader = dataloader(val_df, tokenizer,batch_size=1000)\n",
        "    test_dataloader = dataloader(test_df, tokenizer,batch_size=1000)\n",
        "\n",
        "    return train_dataloader, val_dataloader, test_dataloader"
      ],
      "metadata": {
        "id": "ZRFGB3LJesVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader,val_dataloader,test_dataloader = create_train_val_test_arrays(df[\"password\"], BertDataloader, berttokenizer)"
      ],
      "metadata": {
        "id": "tAYbdduyeu8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df['password'][0])\n",
        "token,labels = train_dataloader[0]"
      ],
      "metadata": {
        "id": "hLbplBb8gnWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token"
      ],
      "metadata": {
        "id": "-KeYkcxLqLZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, temp_df = train_test_split(df['password'], test_size=0.2, random_state=42)\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)"
      ],
      "metadata": {
        "id": "H45xzYelyjO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = create_bert_dataset(train_df, berttokenizer, seq_length, batch_size=100)\n",
        "val_dataset = create_bert_dataset(val_df, berttokenizer, seq_length, batch_size=100)\n",
        "test_dataset = create_bert_dataset(test_df, berttokenizer, seq_length, batch_size=100)"
      ],
      "metadata": {
        "id": "NkmawOkAxAHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelo"
      ],
      "metadata": {
        "id": "wi9T7ZWwTJco"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = TFDistilBertForMaskedLM.from_pretrained('distilbert-base-cased', from_pt=True)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(2e-5),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "yzkj6eSjrQDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history=model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=5,\n",
        "    )"
      ],
      "metadata": {
        "id": "5w_4D9yOrUp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the path to save the model in your Google Drive\n",
        "# You might want to change 'bert_password_model' to a specific name\n",
        "model_save_path = '/content/drive/MyDrive/my_models/bert_password_model.keras\"\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs(model_save_path, exist_ok=True)\n",
        "\n",
        "# Save the model and tokenizer\n",
        "# Use save_pretrained for Hugging Face models\n",
        "model.save_pretrained(model_save_path)\n",
        "berttokenizer.save_pretrained(model_save_path)\n",
        "\n",
        "print(f\"BERT model and tokenizer saved to: {model_save_path}\")"
      ],
      "metadata": {
        "id": "VocdFJl23Dlf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Métricas"
      ],
      "metadata": {
        "id": "964_M2FQTK0T"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "099f8bc6"
      },
      "source": [
        "def evaluate_password_with_bert(password, model, tokenizer, seq_length=32):\n",
        "    tokenized_password = tokenizer(password,\n",
        "                                   padding='max_length',\n",
        "                                   truncation=True,\n",
        "                                   max_length=seq_length,\n",
        "                                   return_tensors='tf',\n",
        "                                   split_special_tokens=True\n",
        "                                  )\n",
        "\n",
        "    input_ids = tokenized_password['input_ids']\n",
        "    attention_mask = tokenized_password['attention_mask']\n",
        "    sep_index = tf.where(input_ids == tokenizer.sep_token_id)[0][1].numpy()\n",
        "\n",
        "    if sep_index <= 1:\n",
        "        print(\"Password too short to mask a meaningful token.\")\n",
        "        return None\n",
        "\n",
        "    probabilities = []\n",
        "    for mask_index in range(1, sep_index):\n",
        "\n",
        "        original_token_id = input_ids[0, mask_index].numpy()\n",
        "        original_token = tokenizer.decode([original_token_id])\n",
        "\n",
        "        masked_input_ids = tf.identity(input_ids)\n",
        "        masked_input_ids = tf.tensor_scatter_nd_update(masked_input_ids, [[0, mask_index]], [tokenizer.mask_token_id])\n",
        "\n",
        "        predictions = model({'input_ids': masked_input_ids, 'attention_mask': attention_mask})\n",
        "        masked_token_logits = predictions.logits[0, mask_index, :]\n",
        "\n",
        "        probs = tf.nn.softmax(masked_token_logits).numpy()\n",
        "\n",
        "        probability_of_original_token = probs[original_token_id]\n",
        "        probabilities.append(probability_of_original_token)\n",
        "\n",
        "        print(f\"Masked token at index {mask_index}: '{original_token}' (ID: {original_token_id}) - Probability: {probability_of_original_token:.4f}\")\n",
        "\n",
        "\n",
        "    return probabilities"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def plot_bert_probability_heatmap(password, probabilities, tokenizer, seq_length=32):\n",
        "    tokenized_password = tokenizer(password,\n",
        "                                   padding='max_length',\n",
        "                                   truncation=True,\n",
        "                                   max_length=seq_length,\n",
        "                                   split_special_tokens=True\n",
        "                                  )\n",
        "    input_ids = tokenized_password['input_ids']\n",
        "    tokens = [tokenizer.decode([token_id]) for token_id in input_ids]\n",
        "\n",
        "\n",
        "    sep_index = input_ids.index(tokenizer.sep_token_id)\n",
        "    display_tokens = tokens[1:sep_index]\n",
        "    display_probabilities = probabilities\n",
        "\n",
        "\n",
        "    if not display_tokens:\n",
        "        print(\"No tokens to display.\")\n",
        "        return\n",
        "\n",
        "    heatmap_data = np.array(display_probabilities).reshape(1, -1)\n",
        "\n",
        "    plt.figure(figsize=(len(display_tokens) * 0.7, 2))\n",
        "    plt.imshow(heatmap_data, cmap='viridis_r', aspect='auto', vmin=0.0, vmax=1.0)\n",
        "    plt.colorbar(label='Probability of Original Token (0-1)')\n",
        "\n",
        "    for i in range(len(display_tokens)):\n",
        "        text_color = 'white' if heatmap_data[0, i] < 0.5 else 'black'\n",
        "        plt.text(i, 0, display_tokens[i], ha='center', va='center', color=text_color, fontsize=10)\n",
        "\n",
        "    plt.title(f\"BERT Probability Heatmap for: {password}\")\n",
        "    plt.yticks([])\n",
        "    plt.xticks([])\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "K10WkoRp10JX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_password = \"password123\"\n",
        "average_prob = evaluate_password_with_bert(user_password, model, berttokenizer, seq_length)\n",
        "plot_bert_probability_heatmap(user_password, average_prob, berttokenizer, seq_length)\n",
        "\n",
        "user_password = \"aNXeuj~ouU\"\n",
        "average_prob = evaluate_password_with_bert(user_password, model, berttokenizer, seq_length)\n",
        "plot_bert_probability_heatmap(user_password, average_prob, berttokenizer, seq_length)\n",
        "\n",
        "user_password = \"ThisIsAStrongPassword123!\"\n",
        "average_prob = evaluate_password_with_bert(user_password, model, berttokenizer, seq_length)\n",
        "plot_bert_probability_heatmap(user_password, average_prob, berttokenizer, seq_length)"
      ],
      "metadata": {
        "id": "fEcXCK_w2jAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ZXCVBN"
      ],
      "metadata": {
        "id": "GgokzsK6KQAc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def zxcvbneficar(senhas):\n",
        "  for s in senhas:\n",
        "    resultado = zxcvbn(s)\n",
        "    print(resultado)\n",
        "    print(f\"Senha: {s}\")\n",
        "    print(f\"  - Pontuação (0 a 4): {resultado['score']}\")\n",
        "    print(f\"  - Feedback: {resultado['feedback']['warning'] or 'Nenhum aviso'}\")\n",
        "    print(f\"  - Sugestões: {resultado['feedback']['suggestions'] or 'Nenhuma'}\")\n",
        "    print(f\"  - Tempo de quebra: {resultado['crack_times_display']['offline_fast_hashing_1e10_per_second']}\")\n",
        "    print(\"--------------------------------------------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "senhas = [\n",
        "    \"password12345\",\n",
        "    \"ThisIsAStrongPassword123!\",\n",
        "    \"aNXeuj~ouU\"\n",
        "]\n",
        "\n",
        "zxcvbneficar(senhas)"
      ],
      "metadata": {
        "id": "ejIqle_UJuNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for senha in senhas:\n",
        "  print(senha)\n",
        "  resultado = zxcvbn(senha)\n",
        "  for atributo in resultado:\n",
        "    print(atributo,resultado[atributo])"
      ],
      "metadata": {
        "id": "LmOr_iI-3-Oe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for senha in senhas:\n",
        "  print(senha)\n",
        "  resultado = zxcvbn(senha)\n",
        "  for sequence in resultado[\"sequence\"]:\n",
        "    print(sequence)"
      ],
      "metadata": {
        "id": "2KRnanxz3bFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# iterativamente fazendo uma senha melhor"
      ],
      "metadata": {
        "id": "CA_HC18f22cY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string"
      ],
      "metadata": {
        "id": "Zt_vK-lGJL--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "possible_characters = list(string.ascii_letters + string.digits + string.punctuation)"
      ],
      "metadata": {
        "id": "M1v1jFSVJPYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_better_password(password, metric, tokenizer, seq_length=32):\n",
        "  result = zxcvbn(password)\n",
        "  while result['score'] < 4:\n",
        "    score = metric.get_weighted_scores(password,weight=[1,1])\n",
        "    modified = False\n",
        "    for sequence in result[\"sequence\"]:\n",
        "      print(\"found:\",sequence[\"token\"],\"with\",sequence['pattern'])\n",
        "      if sequence['pattern'] in ['dictionary','repeat']:\n",
        "        string = sequence['token']\n",
        "        first_char_pos = password.find(string)\n",
        "        last_char_pos = first_char_pos + len(string)\n",
        "        best = np.argmax(score[first_char_pos:last_char_pos])\n",
        "        best_position = first_char_pos + best\n",
        "        alternatives = metric.get_rarer_alternatives(password[:best_position])\n",
        "        alternatives = [a for a in alternatives if a in possible_characters]\n",
        "        if alternatives:\n",
        "          modified = True\n",
        "          password = password[:best_position] + np.random.choice(alternatives) + password[best_position+1:]\n",
        "      else:\n",
        "        probs = metric.get_probs(password)[0]\n",
        "        possible_characters_score = []\n",
        "        for character in possible_characters:\n",
        "          possible_characters_score.append(1-probs[tokenizer.char_to_int[character]])\n",
        "        possible_characters_score = np.array(possible_characters_score)\n",
        "        possible_characters_score = possible_characters_score / possible_characters_score.sum()\n",
        "        password += np.random.choice(possible_characters,p=possible_characters_score)\n",
        "        modified = True\n",
        "      print(\"changed password to:\", password)\n",
        "      result = zxcvbn(password)\n",
        "    if not modified:\n",
        "        break\n",
        "  return password\n"
      ],
      "metadata": {
        "id": "ulTQIIe6-S9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(make_better_password(\"password12345\",rnnmetrics,rnntokenizer))"
      ],
      "metadata": {
        "id": "D0-yq9e6IuJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(make_better_password(\"ThisIsAStrongPassword123!\",rnnmetrics,rnntokenizer))"
      ],
      "metadata": {
        "id": "yrZaFZu9OT5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(make_better_password(\"aNXeuj~ouU\",rnnmetrics,rnntokenizer))"
      ],
      "metadata": {
        "id": "Cnh-XT_MOYxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(make_better_password(\"ilovepassword\",rnnmetrics,rnntokenizer))"
      ],
      "metadata": {
        "id": "mGa4d3JpQeHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(make_better_password(\"123123123123\",rnnmetrics,rnntokenizer))"
      ],
      "metadata": {
        "id": "0rQWuRoe3QX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(make_better_password(\"deumaoito\",rnnmetrics,rnntokenizer))"
      ],
      "metadata": {
        "id": "DK5CM5JkJ3ir"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}