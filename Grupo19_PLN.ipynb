{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMB0z3Lvo8wtUSJXSvawZAy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/drawnator/PLN-grupo-19/blob/main/Grupo19_PLN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ferramenta otimizadora de aleatoriedade em senhas\n",
        "---\n",
        "Modelo de linguagem que auxilia a tornar senhas mais fortes, analisando a entrada e dando sugestões que tornariam a senha mais improvável de adivinhar.\n",
        "\n",
        "Assuntos:\n",
        "- analise de frequencia\n",
        "- masked language model\n",
        "\n",
        "técnologia:\n",
        "- bert\n",
        "- RNN\n",
        "- arvore de decisão\n",
        "- senha aleatória"
      ],
      "metadata": {
        "id": "hX_YXyR6EBrn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Dense, Dropout, SimpleRNN\n",
        "from keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "_RQAO_HBKYuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# masked language mode"
      ],
      "metadata": {
        "id": "rGigW6AgJasc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## read and split dataframe"
      ],
      "metadata": {
        "id": "YNVsgOOUL0ug"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4b66dd42"
      },
      "source": [
        "# RECOMENDO FORTEMENTE BAIXAR E ARRASTAR MANUALMENTE ATÉ ARTIGOS, ISSO AQUI DEMORA MT \\/\n",
        "url = \"https://github.com/brannondorsey/naive-hashcat/releases/download/data/rockyou.txt\"\n",
        "# response = requests.get(url)\n",
        "# data = response.text\n",
        "#..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"rockyou.txt\", \"r\", encoding='latin-1') as f:\n",
        "  df = pd.DataFrame(f.readlines(), columns=['password'])"
      ],
      "metadata": {
        "id": "EoCAcifQKhOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['password'] = df['password'].str.replace('\\n', '')"
      ],
      "metadata": {
        "id": "gex9sDQrNTPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if not os.path.exists(\"rockyou.csv\"):\n",
        "#   df.to_csv(\"rockyou.csv\")"
      ],
      "metadata": {
        "id": "3FZVEsICJ583"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5fxzZdWL7YD",
        "outputId": "c4c9aefe-3cd0-474d-c11a-145e8f95d4b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(14344391, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## tokenization"
      ],
      "metadata": {
        "id": "0f0dF3CQNf5f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#https://colab.research.google.com/drive/1Suv_JhRhoYNOCHtrGQwqZQO18nMXHEfq?usp=sharing\n",
        "def mask_tokens(inputs, tokenizer, mlm_probability=0.10):\n",
        "    inputs = np.array(inputs)\n",
        "    labels = np.copy(inputs)\n",
        "\n",
        "    rand = np.random.rand(*inputs.shape)\n",
        "    mask_arr = (rand < mlm_probability)\n",
        "\n",
        "    special_tokens = [tokenizer.cls_token_id, tokenizer.sep_token_id]\n",
        "    for special_id in special_tokens:\n",
        "        mask_arr[inputs == special_id] = False\n",
        "\n",
        "    inputs[mask_arr] = tokenizer.mask_token_id\n",
        "\n",
        "    labels[~mask_arr] = 0\n",
        "\n",
        "    return inputs, labels"
      ],
      "metadata": {
        "id": "JcB4Y-rpNfDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_chars = sorted(list(set(''.join(df['password']))))\n",
        "char_to_int = {char: i for i, char in enumerate(all_chars)}\n",
        "int_to_char = {i: char for char, i in char_to_int.items()}"
      ],
      "metadata": {
        "id": "fVY6zfNDQyRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DummyTokenizer:\n",
        "    def __init__(self, char_to_int,int_to_char,max_length = 32):\n",
        "        self.char_to_int = char_to_int\n",
        "        self.int_to_char = int_to_char\n",
        "        self.mask_token_id = char_to_int['[MASK]']\n",
        "        self.cls_token_id = char_to_int['[CLS]']\n",
        "        self.sep_token_id = char_to_int['[SEP]']\n",
        "        self.pad_token_id = char_to_int['[PAD]']\n",
        "        self.max_length = max_length\n",
        "        self.vocab_size = len(char_to_int)\n",
        "\n",
        "    def character_tokenizer(self,text):\n",
        "      token = []\n",
        "      for i in range(self.max_length):\n",
        "        if (i < len(text)):\n",
        "          token.append(char_to_int[text[i]])\n",
        "        else:\n",
        "          token.append(char_to_int['[PAD]'])\n",
        "      return token"
      ],
      "metadata": {
        "id": "FVDjp6dwT5f3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "char_to_int['[MASK]'] = len(char_to_int)\n",
        "char_to_int['[CLS]'] = len(char_to_int)\n",
        "char_to_int['[SEP]'] = len(char_to_int)\n",
        "char_to_int['[PAD]'] = len(char_to_int)\n",
        "int_to_char = {i: char for char, i in char_to_int.items()}"
      ],
      "metadata": {
        "id": "i0oOgtFOT93d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_tokenizer = DummyTokenizer(char_to_int,int_to_char)"
      ],
      "metadata": {
        "id": "sMTlDRQeUAa6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['tokenized_password'] = df['password'].apply(dummy_tokenizer.character_tokenizer)"
      ],
      "metadata": {
        "id": "58BckWpOSTK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "masked_inputs = []\n",
        "masked_labels = []"
      ],
      "metadata": {
        "id": "B2zCM--gSz0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for tokenized_list in df['tokenized_password'].head().tolist():\n",
        "    inputs, labels = mask_tokens(tokenized_list, dummy_tokenizer)\n",
        "    masked_inputs.append(inputs)\n",
        "    masked_labels.append(labels)"
      ],
      "metadata": {
        "id": "njfdSklaTAde",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "outputId": "6d56d5e0-5038-460c-e3ab-d2613112c05f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'mask_tokens' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1231583095.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtokenized_list\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokenized_password'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdummy_tokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mmasked_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmasked_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'mask_tokens' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Splitting data"
      ],
      "metadata": {
        "id": "GkaguFhoY-vs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train, temp = train_test_split(df, test_size=0.2)\n",
        "val, test = train_test_split(df, test_size=0.5)"
      ],
      "metadata": {
        "id": "BgXFl4WJZg2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbde77b8"
      },
      "source": [
        "def create_train_val_test_datasets(df, tokenizer, test_size=0.10, val_size=0.15, batch_size=8,max=None):\n",
        "\n",
        "    if max:\n",
        "        df = df.head(max)\n",
        "\n",
        "    train_df, temp_df = train_test_split(df, test_size=test_size, random_state=42)\n",
        "    val_df, test_df = train_test_split(temp_df, test_size=val_size, random_state=42)\n",
        "\n",
        "    def df_to_dataset(dataframe, tokenizer, batch_size):\n",
        "        tokenized_passwords = dataframe['tokenized_password'].tolist()\n",
        "\n",
        "        masked_inputs = []\n",
        "        masked_labels = []\n",
        "        for tokens in tokenized_passwords:\n",
        "            inputs, labels = mask_tokens(tokens, tokenizer)\n",
        "            masked_inputs.append(inputs)\n",
        "            masked_labels.append(labels)\n",
        "\n",
        "        input_ids = tf.constant(masked_inputs, dtype=tf.int32)\n",
        "        labels = tf.constant(masked_labels, dtype=tf.int32)\n",
        "\n",
        "        dataset = tf.data.Dataset.from_tensor_slices((input_ids, labels))\n",
        "        return dataset.shuffle(1000).batch(batch_size)\n",
        "\n",
        "    train_dataset = df_to_dataset(train_df, tokenizer, batch_size)\n",
        "    val_dataset = df_to_dataset(val_df, tokenizer, batch_size)\n",
        "    test_dataset = df_to_dataset(test_df, tokenizer, batch_size)\n",
        "\n",
        "    return train_dataset, val_dataset, test_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "def create_train_val_test_arrays(df, tokenizer, test_size=0.2, val_size=0.5, max_length=32):\n",
        "    train_df, temp_df = train_test_split(df, test_size=test_size, random_state=42)\n",
        "    val_df, test_df = train_test_split(temp_df, test_size=val_size, random_state=42)\n",
        "\n",
        "    def dataframe_to_arrays(dataframe, tokenizer, max_length):\n",
        "        tokenized_passwords = dataframe['tokenized_password'].tolist()\n",
        "\n",
        "        masked_inputs = []\n",
        "        masked_labels = []\n",
        "\n",
        "        for tokens in tokenized_passwords:\n",
        "          for i in range(1,len(tokens)):\n",
        "            input_seq = tokens[:i]\n",
        "            label = tokens[i]\n",
        "            input_seq += [tokenizer.pad_token_id] * (max_length - len(input_seq))\n",
        "            masked_inputs.append(input_seq)\n",
        "            masked_labels.append(label)\n",
        "\n",
        "        return np.array(masked_inputs), np.array(masked_labels)\n",
        "\n",
        "    train_inputs, train_labels = dataframe_to_arrays(train_df, tokenizer, max_length)\n",
        "    val_inputs, val_labels = dataframe_to_arrays(val_df, tokenizer, max_length)\n",
        "    test_inputs, test_labels = dataframe_to_arrays(test_df, tokenizer, max_length)\n",
        "\n",
        "    return train_inputs, train_labels, val_inputs, val_labels, test_inputs, test_labels"
      ],
      "metadata": {
        "id": "0bE3jvF_fxIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds, val_ds, test_ds = create_train_val_test_datasets(df, dummy_tokenizer, batch_size=8)"
      ],
      "metadata": {
        "id": "raIaQE6mfXfN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "collapsed": true,
        "outputId": "fd647c6f-3676-40e5-fe12-df31cf102398"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'create_train_val_test_datasets' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1893874762.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_train_val_test_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdummy_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'create_train_val_test_datasets' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_inputs, train_labels, val_inputs, val_labels, test_inputs, test_labels = create_train_val_test_arrays(df, dummy_tokenizer)"
      ],
      "metadata": {
        "id": "2IHv8Tv8gndR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# defining a model\n",
        "TODO: modelo RNN mas train label usados no treinamento do bert, opções:\n",
        "- mudar modelo a baixo para fazer fine tunning do bert\n",
        "- mudar mask_tokens para gerar uma entrada e saida condizente com um problema RNN\n"
      ],
      "metadata": {
        "id": "zwoaQJ2obmui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#https://colab.research.google.com/drive/1mts5E3yAd1irLzS7Ei6UtwbG773C87DB?usp=sharing\n",
        "model = Sequential([\n",
        "    Embedding(\n",
        "        input_dim=dummy_tokenizer.vocab_size,\n",
        "        output_dim=100,\n",
        "        input_shape=(dummy_tokenizer.max_length,)),\n",
        "    SimpleRNN(64, return_sequences=False),\n",
        "    Dropout(0.5),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(dummy_tokenizer.vocab_size, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "M9EaU8RZc8cW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    train_inputs,\n",
        "    train_labels,\n",
        "    validation_data=(val_inputs, val_labels),\n",
        "    epochs=3,\n",
        "    batch_size=256,\n",
        "    verbose=1)"
      ],
      "metadata": {
        "id": "3no88mxsehfx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}